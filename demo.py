import argparse
import os
from glob import glob

import pyrootutils

root = pyrootutils.setup_root(
    search_from=__file__,
    indicator=[".git", "pyproject.toml", ".sl"],
    pythonpath=True,
    dotenv=True,
)

import cv2
import numpy as np
import torch
from sam_3d_body import load_sam_3d_body, SAM3DBodyEstimator
from tools.vis_utils import visualize_sample_together
from tqdm import tqdm


def main(args):
    if args.output_folder == "":
        output_folder = os.path.join("./output", os.path.basename(args.image_folder))
    else:
        output_folder = args.output_folder

    os.makedirs(output_folder, exist_ok=True)

    mhr_path = args.mhr_path or os.environ.get("SAM3D_MHR_PATH", "")
    detector_path = args.detector_path or os.environ.get("SAM3D_DETECTOR_PATH", "")
    segmentor_path = args.segmentor_path or os.environ.get("SAM3D_SEGMENTOR_PATH", "")
    fov_path = args.fov_path or os.environ.get("SAM3D_FOV_PATH", "")

    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    model, model_cfg = load_sam_3d_body(
        args.checkpoint_path, device=device, mhr_path=mhr_path
    )

    human_detector, human_segmentor, fov_estimator = None, None, None
    if args.detector_name:
        from tools.build_detector import HumanDetector
        human_detector = HumanDetector(
            name=args.detector_name, device=device, path=detector_path
        )
    if len(segmentor_path):
        from tools.build_sam import HumanSegmentor
        human_segmentor = HumanSegmentor(
            name=args.segmentor_name, device=device, path=segmentor_path
        )
    if args.fov_name:
        from tools.build_fov_estimator import FOVEstimator
        fov_estimator = FOVEstimator(name=args.fov_name, device=device, path=fov_path)

    estimator = SAM3DBodyEstimator(
        sam_3d_body_model=model,
        model_cfg=model_cfg,
        human_detector=human_detector,
        human_segmentor=human_segmentor,
        fov_estimator=fov_estimator,
    )

    image_extensions = [
        "*.jpg", "*.jpeg", "*.png", "*.gif",
        "*.bmp", "*.tiff", "*.webp",
    ]

    images_list = sorted([
        image
        for ext in image_extensions
        for image in glob(os.path.join(args.image_folder, ext))
    ])

    for image_path in tqdm(images_list):
        img = cv2.imread(image_path)
        outputs = estimator.process_one_image(
            image_path,
            bbox_thr=args.bbox_thresh,
            use_mask=args.use_mask,
        )

        image_name = os.path.basename(image_path).split(".")[0]

        glb_path = visualize_sample_together(
            img, outputs, estimator.faces,
            outdir=output_folder,
            image_name=image_name
        )

        print(f"Saved 3D model â†’ {glb_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--image_folder", required=True)
    parser.add_argument("--output_folder", default="")
    parser.add_argument("--checkpoint_path", required=True)
    parser.add_argument("--detector_name", default="vitdet")
    parser.add_argument("--segmentor_name", default="sam2")
    parser.add_argument("--fov_name", default="moge2")
    parser.add_argument("--detector_path", default="")
    parser.add_argument("--segmentor_path", default="")
    parser.add_argument("--fov_path", default="")
    parser.add_argument("--mhr_path", default="")
    parser.add_argument("--bbox_thresh", default=0.8, type=float)
    parser.add_argument("--use_mask", action="store_true", default=False)
    args = parser.parse_args()
    main(args)
